{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip install pytorch_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import torch\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from transformers import BertForSequenceClassification, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init hardware resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "  \n",
    "  # Tell PyTorch to use the GPU.\n",
    "  device = torch.device(\"cuda\")\n",
    "\n",
    "  print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "  print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "  # If not...\n",
    "  print('No GPU available, using the CPU instead.')\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>026f4c15-37a9-459d-a944-03bde29a5c59</td>\n",
       "      <td>அமைச்சின் பணிகளை முன்னெடுப்பதற்கு கௌர அமைச்சர்...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0df3b73b-b08a-4357-bd6d-3bd94f8d4e58</td>\n",
       "      <td>இவ்வமைச்சு இல 40  புத்கமுவ வீதி  இராஜகிரிய எனு...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  026f4c15-37a9-459d-a944-03bde29a5c59   \n",
       "1  0df3b73b-b08a-4357-bd6d-3bd94f8d4e58   \n",
       "\n",
       "                                                news  \n",
       "0  அமைச்சின் பணிகளை முன்னெடுப்பதற்கு கௌர அமைச்சர்...  \n",
       "1  இவ்வமைச்சு இல 40  புத்கமுவ வீதி  இராஜகிரிய எனு...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtypes = {\n",
    "    'id': str,\n",
    "    'news': str\n",
    "}\n",
    "\n",
    "csv_file = '../../data/news_with_header.csv'\n",
    "df = pd.read_csv(csv_file, dtype=dtypes)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the pretained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The name of the folder containing the model files.\n",
    "pretrained_model = 'bert-base-multilingual-uncased'\n",
    "\n",
    "# Load our fine-tuned model, and configure it to return the \"hidden states\",\n",
    "# from which we will be taking our text embeddings.\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    pretrained_model,\n",
    "    output_hidden_states=True,  # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# Load the tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "# Copy the model to the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_to_embedding(tokenizer, model, in_text):\n",
    "    #   Uses the provided BERT `model` and `tokenizer` to generate a vector \n",
    "    #   representation of the input string, `in_text`.\n",
    "    #   Returns the vector stored as a numpy ndarray.\n",
    "  \n",
    "    # ===========================\n",
    "    #    STEP 1: Tokenization\n",
    "    # ===========================\n",
    "    tokens = tokenizer.tokenize(in_text)\n",
    "    if (len(tokens) > 510):\n",
    "        tokens = tokens[:128] + tokens[-382:]\n",
    "\n",
    "    MAX_LEN = 512\n",
    "\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Truncate the sentence to MAX_LEN if necessary.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end. (After truncating!)\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    input_ids = tokenizer.encode(\n",
    "        tokens,                   # Sentence to encode.\n",
    "        add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "        max_length=MAX_LEN,       # Truncate all sentences.\n",
    "    )\n",
    "    \n",
    "    # Pad our input tokens. Truncation was handled above by the `encode`\n",
    "    # function, which also makes sure that the `[SEP]` token is placed at the\n",
    "    # end *after* truncating.\n",
    "    # Note: `pad_sequences` expects a list of lists, but we only have one\n",
    "    # piece of text, so we surround `input_ids` with an extra set of brackets.\n",
    "    results = pad_sequences([input_ids], maxlen=MAX_LEN, dtype=\"long\",\n",
    "                            truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "    # Remove the outer list.\n",
    "    input_ids = results[0]\n",
    "    \n",
    "    # Create attention masks\n",
    "    attn_mask = [int(i > 0) for i in input_ids]\n",
    "    \n",
    "    # Cast to tensors.\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attn_mask = torch.tensor(attn_mask)\n",
    "\n",
    "    # Add an extra dimension for the \"batch\" (even though there is only one\n",
    "    # input in this batch.)\n",
    "    input_ids = input_ids.unsqueeze(0)\n",
    "    attn_mask = attn_mask.unsqueeze(0)\n",
    "\n",
    "    # ===========================\n",
    "    #    STEP 2: BERT Model\n",
    "    # ===========================\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Copy the inputs to the GPU\n",
    "    # Note -- I got stuck here for a while because I didn't assign the result\n",
    "    # back to the variable! Geez!\n",
    "    input_ids = input_ids.to(device)\n",
    "    attn_mask = attn_mask.to(device)\n",
    "\n",
    "    # Telling the model not to build the backwards graph will make this\n",
    "    # a little quicker.\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, return hidden states and predictions.\n",
    "        # This will return the logits rather than the loss because we have\n",
    "        # not provided labels.\n",
    "        logits, encoded_layers = model(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=attn_mask,\n",
    "            return_dict=False)\n",
    "\n",
    "    # Retrieve our sentence embedding--take the `[CLS]` embedding from the final\n",
    "    # layer.\n",
    "    layer_i = 12 # The last BERT layer before the classifier.\n",
    "    batch_i = 0  # Only one input in the batch.\n",
    "    token_i = 0  # The first token, corresponding to [CLS]\n",
    "\n",
    "    # Grab the embedding.\n",
    "    vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "    \n",
    "    # Move to the CPU and convert to numpy ndarray.\n",
    "    vec = vec.detach().cpu().numpy()\n",
    "\n",
    "    return(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting embedding for news:\n",
      "\n",
      " அரச உத்தியோகத்தர்கள் மொழித் தேர்ச்சி பெற்றுக் கொள்வதற்கு அமைவாக முன்வைக்கப்பட்டுள்ள உத்தியோகத்தர் குழு அறிக்கை சிபாரிசுகளை நடைமுறைப்படுத்தல்\n",
      "\n",
      "Done. Embedding shape: (768,)\n",
      "\n",
      "Done. news embedding vector:\n",
      " [-8.16217661e-02  1.38339503e-02  2.86057275e-02 -3.13600665e-03\n",
      " -2.15890378e-01  1.07254073e-01 -8.52226615e-02  1.71499103e-02\n",
      " -1.95386875e+00 -1.36038959e-02 -4.05059606e-02 -1.08759195e-01\n",
      "  6.14710450e-02  6.89011738e-02  9.64766592e-02  1.60595238e-01\n",
      " -2.40740031e-02 -1.11191869e-02  4.91555408e-02 -1.79351699e-02\n",
      " -2.90736314e-02  1.41032159e-01  7.86092281e-02 -8.89550447e-02\n",
      "  6.35270834e-01 -2.79472023e-03  1.31536990e-01 -3.50437090e-02\n",
      " -2.09349632e+00  3.91056463e-02 -2.51639366e-01  2.48275846e-02\n",
      " -1.74585786e-02 -1.33073270e-01 -1.50353406e-02 -1.19470246e-02\n",
      " -1.14890188e-02  1.58056426e+00 -6.75819516e-02 -3.13006416e-02\n",
      "  3.43747735e-02  4.67215478e-03 -8.61967131e-02  9.16192159e-02\n",
      " -2.98888981e-02 -3.70589830e-02 -2.96759903e-02  7.06345588e-03\n",
      "  3.74627113e-02 -4.47248146e-02  2.59104371e-02  1.08500674e-01\n",
      " -5.66889793e-02  4.53094840e-02 -7.09204599e-02  7.73606896e-02\n",
      " -8.27200562e-02  5.89494407e-03  2.37459540e-02  1.02082647e-01\n",
      "  1.96577156e+00  1.84059337e-01 -2.59421505e-02 -4.15363088e-02\n",
      " -1.29653886e-03  7.27743655e-02  3.89977098e-02  7.18082190e-02\n",
      "  1.10383891e-02 -1.30855113e-01 -5.74968234e-02  7.31190071e-02\n",
      " -1.48816407e-03 -9.90479961e-02 -9.84387919e-02 -1.39203012e-01\n",
      "  3.79809827e-01  1.69294663e-02 -6.01162575e-03 -2.51837242e-02\n",
      "  3.65790501e-02 -2.86789201e-02 -1.15023926e-01 -5.46999387e-02\n",
      " -2.87490577e-01  4.21800464e-02  1.43814832e-01 -3.48822549e-02\n",
      " -1.34127140e-01 -1.43337488e-01  3.32146943e-01 -1.37252674e-01\n",
      "  3.54458615e-02  8.59045535e-02  1.02096610e-01  1.52955158e-02\n",
      " -1.79912031e-01  1.95191026e+00 -1.22270219e-01  1.52716756e-01\n",
      " -2.10900903e-02  1.94832638e-01 -2.03601662e-02 -7.12560266e-02\n",
      "  1.64228961e-01 -1.06539242e-02 -1.56769454e-02  1.09686609e-02\n",
      " -1.33614410e-02  5.98081537e-02  6.28951192e-01 -3.07723403e-01\n",
      " -3.32385451e-02 -1.90885931e-01  7.08297566e-02  2.38456428e-02\n",
      "  6.39204010e-02 -4.47853915e-02 -3.10283452e-02  1.27317920e-01\n",
      "  2.23422907e-02  1.83660626e+00 -4.15235497e-02 -1.27244323e-01\n",
      " -2.11741969e-01  7.57752880e-02 -1.10140800e-01 -3.33911814e-02\n",
      "  1.85894102e-01 -9.57421064e-02  1.95374638e-02 -9.28242505e-03\n",
      "  1.13069034e+00 -6.98849261e-02  3.36499363e-02 -3.30178440e-02\n",
      " -5.72351664e-02 -8.26540738e-02  1.96965635e-01  1.05390415e-01\n",
      " -3.82616669e-02 -1.90972880e-01 -9.45340395e-02 -7.84919038e-02\n",
      "  7.28437975e-02  1.33871287e-02  1.06418446e-01  1.38356090e-01\n",
      " -2.87558027e-02  3.31115089e-02 -9.47952271e-02  3.95307839e-02\n",
      "  5.57213128e-02 -1.30624279e-01  6.89073652e-02 -8.45797360e-03\n",
      "  1.82702690e-02 -5.68360567e-01 -6.25857525e-03 -3.07416391e+00\n",
      "  7.23678060e-03 -1.80871766e-02  1.26601219e-01  3.23369652e-02\n",
      "  1.24923027e+00 -4.65721637e-03 -1.42684281e-02  6.52785823e-02\n",
      " -1.40940279e-01 -8.97114910e-03 -6.06376305e-02  5.88873588e-02\n",
      " -6.87954128e-02 -7.68532753e-02 -1.58513874e-01  8.99019837e-02\n",
      " -2.99639516e-02 -1.22415781e-01 -8.96326378e-02  9.23112035e-02\n",
      "  1.71496049e-01  1.03029259e-01 -2.51036547e-02  8.25369656e-02\n",
      "  4.14853580e-02 -4.51959409e-02 -6.55140057e-02 -1.99579187e-02\n",
      " -1.47696376e-01 -2.24408954e-01  1.83806226e-01  6.08609244e-02\n",
      " -9.15612802e-02 -2.61438712e-02 -7.59623796e-02 -4.39191312e-02\n",
      "  1.31546453e-01  5.66802323e-02 -8.35306570e-02  7.99542367e-02\n",
      " -9.67851654e-02  7.38707557e-02 -2.43076980e-02  1.46300212e-01\n",
      " -1.49788648e-01 -1.36420652e-01 -2.58120783e-02 -1.16572529e-01\n",
      "  1.19290099e-01  8.58158022e-02  6.55483305e-02 -7.38218874e-02\n",
      " -1.23836771e-01  4.77796681e-02 -1.14941828e-01 -4.02437076e-02\n",
      " -6.13313317e-02 -3.22071165e-02 -1.50728971e-02  1.18285865e-01\n",
      " -7.60683119e-02 -8.29721242e-02  2.44509410e-02  2.52182223e-02\n",
      " -1.57146677e-02  1.70081146e-02 -1.92269757e-02  1.74896065e-02\n",
      " -7.48273134e-02  7.11447001e-02 -9.40539688e-03  6.70114309e-02\n",
      "  2.05490459e-02  6.32313266e-02 -1.87616050e-03 -2.59345204e-01\n",
      "  2.72495821e-02 -4.23372835e-02 -1.25933234e-02  6.68673739e-02\n",
      "  1.72107331e-02 -4.99078035e-02  1.82161584e-01  1.76916495e-02\n",
      "  1.57640851e+00  9.75202844e-02  9.71108675e-03 -1.32339701e-01\n",
      " -1.45453168e-02 -9.65157822e-02  4.43093181e-02 -8.19650143e-02\n",
      " -1.59263718e+00  1.94386184e-01 -2.01621294e-01  7.57897273e-02\n",
      "  1.96335107e-01 -6.70698509e-02 -4.17953059e-02  3.52222212e-02\n",
      " -1.55685604e+00  5.51415794e-02  6.98429495e-02 -1.05933115e-01\n",
      " -2.46243495e-02 -1.26023740e-01  4.20907997e-02 -8.00265372e-02\n",
      " -1.69650793e-01 -8.78703892e-02  6.87830821e-02  1.01907827e-01\n",
      " -1.16169147e-01 -2.87535060e-02 -9.69307870e-03  2.33880058e-02\n",
      "  4.52203304e-03  1.28270373e-01 -7.84934238e-02 -1.66805107e-02\n",
      " -1.25692040e-02 -2.87635595e-01 -1.03204012e-01 -1.22768484e-01\n",
      "  6.64226711e-03 -1.76287405e-02 -1.04332566e-02  3.06980610e-02\n",
      " -6.20067343e-02  3.74856777e-02 -2.84818053e+00  1.04025781e-01\n",
      "  1.84900060e-01 -7.65008926e-02  5.10776415e-02 -5.91135807e-02\n",
      " -9.51768607e-02  8.82451758e-02  3.28124501e-03 -1.42497923e-02\n",
      "  7.04794452e-02 -1.38333976e-01 -7.21045062e-02  4.71049845e-02\n",
      " -8.87669846e-02 -9.09343287e-02  6.98729306e-02  1.51043922e-01\n",
      " -4.55968708e-01  1.28573626e-02 -1.62545061e+00  1.90634504e-02\n",
      " -1.71614617e-01 -8.80660117e-03 -1.16049513e-01 -1.48135021e-01\n",
      "  4.73753884e-02  4.70146537e-03 -1.21871725e-01 -3.30963023e-02\n",
      "  1.06734984e-01  1.00749642e-01  1.01944432e-02  8.99073780e-02\n",
      "  5.62844127e-02  1.78838223e-02 -9.18958113e-02  1.56425953e-01\n",
      " -6.47118017e-02 -1.82763487e-01  3.94232571e-03  1.85464606e-01\n",
      " -4.84972186e-02 -2.51249988e-02 -9.48754847e-02 -8.11287761e-03\n",
      "  1.83075204e-01  4.86254059e-02 -1.09576173e-01  1.52933493e-01\n",
      " -5.91397360e-02  5.33158779e-02 -1.33209527e-01 -8.45276117e-02\n",
      "  3.66659760e-02 -4.59844284e-02 -2.38055345e-02 -1.11565217e-01\n",
      " -3.05652469e-02  2.91155800e-02 -1.19311482e-01 -8.55940431e-02\n",
      " -1.42838940e-01 -1.12709098e-01 -4.11136076e-02 -3.63183469e-02\n",
      " -6.27884120e-02 -1.04119428e-01 -1.40327327e-02  3.44300419e-02\n",
      " -1.08814433e-01 -2.75649056e-02  5.02960980e-02  1.01856694e-01\n",
      "  1.84277445e-02 -2.19225788e+00 -2.79770419e-02 -2.73674056e-02\n",
      " -4.82259654e-02 -6.67610839e-02  1.89642534e-02  6.74613416e-02\n",
      "  9.42847490e-01  1.31589651e-01 -1.50449932e-01 -5.60307838e-02\n",
      "  3.22157815e-02  5.36470003e-02 -4.13088232e-01 -3.35746482e-02\n",
      "  2.33968552e-02  1.00700736e-01 -6.16565421e-02 -4.75094095e-02\n",
      " -9.89035144e-03 -2.99401954e-02  5.22691570e-02 -2.23348644e-02\n",
      " -4.47360724e-02  1.92637742e-03 -2.59305909e-02 -1.02305360e-01\n",
      " -3.15902233e-02 -1.84543729e-01  2.04323530e-02 -6.03160083e-01\n",
      " -4.16211598e-03  5.26981317e-02  2.86637768e-02  8.33205104e-01\n",
      " -1.25209883e-01 -4.60545085e-02  8.76353309e-03  1.56325161e+00\n",
      "  2.22951081e-02 -3.49343792e-02  5.98706156e-02  4.17945027e-01\n",
      "  1.44636512e-01 -7.44125247e-01 -1.96986508e+00  3.31844613e-02\n",
      "  8.68763030e-03 -2.81440592e+00 -8.84614885e-02 -1.49886474e-01\n",
      "  5.70619404e-02  9.51409116e-02 -5.01506031e-01  8.96420851e-02\n",
      "  8.48684087e-02 -1.76887488e+00  2.11382970e-01 -2.52173364e-01\n",
      "  8.50031599e-02 -1.16854392e-01 -5.85853681e-03  1.33592203e-01\n",
      "  1.30341303e+00  2.85501834e-02 -4.47599404e-02 -7.52380863e-02\n",
      " -2.34835371e-02 -2.78627127e-03 -3.50487679e-02 -4.31109481e-02\n",
      "  1.17482781e-01 -2.45617144e-02 -9.06488299e-03 -3.96871045e-02\n",
      "  2.93615870e-02 -4.22875769e-03 -1.03457257e-01 -2.82481313e-03\n",
      " -1.00448214e-01  1.31366611e-01  5.17087579e-02  2.57111597e+00\n",
      "  4.64836434e-02  9.44539756e-02  7.23267496e-02  5.18513024e-02\n",
      "  9.32716429e-02  1.09722376e-01  7.59133771e-02  5.65572381e-02\n",
      "  7.75200576e-02  1.08058795e-01  9.92022455e-03  2.03474617e+00\n",
      " -1.02816924e-01  8.69710520e-02 -1.24435522e-01  9.50093567e-02\n",
      " -1.57319754e-01 -5.38208596e-02 -1.95778310e-02  7.90801570e-02\n",
      "  4.27138247e-02  2.79956646e-02  1.78633004e-01  5.05529106e-01\n",
      " -9.72644314e-02 -4.57170419e-03  5.50818183e-02 -9.23723131e-02\n",
      "  1.08522251e-01  4.03084010e-02 -2.44925261e-01  2.21497566e-02\n",
      " -7.03571290e-02 -8.69173482e-02 -4.28983048e-02  3.16663608e-02\n",
      "  1.24146655e-01  4.30880189e-02 -3.61832976e-02  2.36950874e-01\n",
      "  3.54877077e-02 -3.42810154e-02 -1.00068152e-02 -1.11326106e-01\n",
      "  6.87377676e-02  6.63373992e-02 -1.36002886e+00 -5.21482751e-02\n",
      "  2.12092102e-02 -6.57460690e-02  3.02876346e-02  1.52382362e+00\n",
      "  7.47623593e-02  9.03609544e-02  3.12634483e-02  4.24555056e-02\n",
      " -6.88217059e-02 -5.71333617e-03  1.57602966e+00  1.57891493e-02\n",
      " -1.40910029e-01 -2.67015062e-02 -1.19698755e-02 -3.48793715e-03\n",
      " -3.14308368e-02  7.14509934e-02 -4.22163121e-02 -7.62172565e-02\n",
      "  9.64737684e-02  1.11042149e-01  1.11379072e-01  8.68059248e-02\n",
      "  1.41806901e-01 -1.29482463e-01 -5.60132116e-02 -1.59583271e-01\n",
      "  1.66171610e-01 -2.52562501e-02  2.39236057e-02 -1.24736778e-01\n",
      "  2.10659727e-02  1.58703029e+00 -6.31244183e-02  2.98951566e-02\n",
      "  4.34974059e-02 -8.21017623e-02 -4.94886041e-02 -1.73516572e-03\n",
      "  5.32551594e-02 -1.87127814e-02 -2.69572139e-02  5.22911958e-02\n",
      " -8.03751051e-02 -5.08595109e-02 -1.18585490e-01  1.35669148e+00\n",
      " -5.88515252e-02  7.01927617e-02 -3.85051668e-01  6.55398071e-01\n",
      "  8.01040977e-02 -1.34910969e-02 -2.73071826e-01 -1.57042220e-02\n",
      "  5.63352853e-02  8.38173926e-02  4.24716994e-03  4.72847521e-01\n",
      "  1.33569241e-02 -2.17450559e-02 -2.19235241e-01 -1.16593897e-01\n",
      "  1.14043094e-01 -2.26196814e-02 -5.93639351e-02 -2.46541190e+00\n",
      " -5.21772802e-02 -3.87715213e-02  5.80214262e-02 -7.17713684e-03\n",
      "  2.10477948e-01  6.33672178e-02  5.89709133e-02  5.33252396e-02\n",
      " -9.22325999e-02 -3.94339412e-02  5.47711849e-02  4.30590734e-02\n",
      " -4.21682410e-02 -6.67214915e-02 -1.07774682e-01 -4.38865721e-02\n",
      "  3.57192382e-02  1.47663802e-01  6.86849952e-02  2.33013988e-01\n",
      "  9.75372419e-02  1.55258760e-01  2.68008616e-02  5.55900335e-02\n",
      " -9.65327173e-02 -1.86499879e-01 -4.90004718e-02  1.64308771e-02\n",
      "  7.00406730e-02  2.84652486e-02 -1.29559338e-02 -2.98844203e-02\n",
      "  7.65844062e-02 -1.95723668e-01  2.23703869e-03  4.86069441e-01\n",
      " -1.06449318e+00  9.49483290e-02 -4.67566215e-02  1.71901099e-02\n",
      "  1.43163466e+00  7.78230280e-02 -1.34031177e-02  9.13087279e-03\n",
      "  2.80370787e-02  1.20854400e-01 -7.48408809e-02  2.39961222e-02\n",
      " -3.88726257e-02  9.86979455e-02  1.49302006e-01 -7.24775940e-02\n",
      "  4.18687612e-02  7.90344477e-02  4.71854210e-02 -6.17419854e-02\n",
      "  1.49534002e-01 -8.71188939e-02  8.84886757e-02  1.19719133e-01\n",
      "  5.30847255e-03  2.35169083e-02 -7.24555627e-02  7.65369833e-02\n",
      " -2.53456831e-03 -4.85090539e-03 -1.79374695e+00 -1.62387878e-01\n",
      " -3.68280672e-02  3.10962442e-02 -6.99547604e-02 -1.61176980e-01\n",
      "  1.18754947e+00  1.99377201e-02 -1.41501188e-01 -1.31264329e-04\n",
      "  1.56682089e-01  1.91592073e+00 -8.75269175e-02  6.03469387e-02\n",
      "  1.30088061e-01 -3.11798155e-01 -2.66515538e-02 -2.77159065e-02\n",
      " -5.69007397e-02 -4.22507301e-02 -1.96042609e+00  5.67462929e-02\n",
      "  4.37441655e-02 -7.28054047e-02 -1.65564549e+00 -2.23098218e-01\n",
      " -4.87195328e-02 -7.93000385e-02 -1.79280862e-02 -2.51417309e-02\n",
      " -6.26412779e-03  1.37372226e-01 -6.04697764e-02  1.14607923e-01\n",
      " -7.97445327e-03  1.34175718e-02  1.68421119e-03 -1.87664032e-01\n",
      "  1.35849249e+00 -2.21483779e+00  1.06234685e-01 -1.77023590e-01\n",
      "  7.38408193e-02 -5.35371825e-02  2.07190569e-02  1.21512629e-01\n",
      "  1.78590560e+00 -3.59397084e-02  1.18543513e-01 -8.60763490e-02\n",
      "  6.33891746e-02  2.04742029e-02 -1.99071355e-02 -8.29695538e-02\n",
      " -8.96702707e-02  4.18819487e-04  5.85691035e-02 -3.69392782e-02\n",
      " -1.52605213e-02  1.25299290e-01 -1.52718956e-02  1.00592665e-01\n",
      " -3.56074795e-02 -1.76796424e+00  3.14214677e-02 -5.34994379e-02\n",
      "  5.75105473e-03 -1.03592873e-01  2.30863905e+00 -1.73127018e-02\n",
      " -2.75131762e-02  5.17285839e-02  1.07195102e-01 -1.61269680e-02\n",
      "  1.53351232e-01 -1.41658157e-01 -1.52133942e-01  1.17814317e-02\n",
      "  1.00628287e-02 -7.44812787e-01  4.69803438e-02 -1.27705052e-01\n",
      "  8.74543786e-02  2.58450508e-02  4.67315316e-03 -5.22986017e-02\n",
      " -1.90304726e-01  1.68023873e-02  3.45616266e-02 -7.96139315e-02\n",
      " -5.75897768e-02 -5.04126474e-02 -1.87806636e-02 -2.04563141e-03\n",
      " -7.55023584e-03  6.30004480e-02 -1.12555921e-02  9.84057412e-02\n",
      " -1.66651201e+00  4.74315248e-02  1.00033134e-02  1.58305061e+00\n",
      "  3.37980613e-02 -1.66606724e-01  1.41621816e+00  1.21406451e-01\n",
      "  3.98813281e-03  4.60571200e-02 -8.19931924e-02 -7.31831864e-02\n",
      "  1.10504583e-01 -6.18005581e-02 -1.89296019e+00 -2.25544047e+00\n",
      " -2.32259110e-02  2.83713527e-02  3.57187986e-02 -2.04444826e-02\n",
      "  1.30485706e-02  7.10634515e-04 -7.33118430e-02 -1.60680458e-01\n",
      " -3.41655090e-02  4.57877219e-02 -1.06590763e-01  3.09678651e-02\n",
      "  2.34513283e-02  1.20454752e+00  3.94773334e-02  2.06414461e+00\n",
      " -8.55328739e-02 -8.77407193e-03  1.37010455e-01 -7.79464096e-03\n",
      " -1.18993640e-01 -4.35166620e-02 -5.62672317e-03 -4.03071046e-02]\n"
     ]
    }
   ],
   "source": [
    "# Get the onw news from the list.\n",
    "input_text = df['news'].iloc[10]\n",
    "\n",
    "print('Getting embedding for news:\\n\\n', input_text)\n",
    "\n",
    "# Use the BERT model and tokenizer to generate an embedding for `input_text`.\n",
    "vec = news_to_embedding(tokenizer, model, input_text)\n",
    "\n",
    "print('\\nDone. Embedding shape:', str(vec.shape))\n",
    "print('\\nDone. news embedding vector:\\n', str(vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch process news to extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the set of embeddings.\n",
    "embeddings = []\n",
    "\n",
    "num_news = len(df['news'])\n",
    "\n",
    "print('Generating news embeddings for all {:,} news...'.format(num_news))\n",
    "\n",
    "# For each row of the dataframe...\n",
    "for idx, row in tqdm(df.iterrows()):\n",
    "  # Vectorize this news.\n",
    "  vec = news_to_embedding(tokenizer, model, row.news)\n",
    "\n",
    "  # Store the embeddings.\n",
    "  embeddings.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of vectors into a 2D array.\n",
    "vecs = np.stack(embeddings)\n",
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '../../data/vectors/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "  os.makedirs(output_dir)\n",
    "\n",
    "# Use numpy to write out the matrix of embeddings.\n",
    "print(f'Saving vec to: {output_dir}embeddings.npy')\n",
    "np.save(f'{output_dir}embeddings.npy', vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
